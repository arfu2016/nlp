1. I built a linear regression model showing 95% confidence interval. Does it mean that there is a 95% chance that my model coefficients are the true estimate of the function I am trying to approximate? (Hint: It actually means 95% of the time…)

系数的95%置信区间(m加减s）表明真实值有95%的可能性落在这个区间内，但这并不意味着，点估计m有95%的可能性是正确的。

系数的估计（被认为是一个随机变量）的分布一般认为是正态分布，两边各拿出2.5%的面积，中间就是95%的置信区间。

2. What is a similarity between Hadoop file system and k-nearest neighbor algorithm? (Hint: ‘lazy’)

Hadoop不懂，只知道用了map、reduce技术，knn倒是了解，找离要分类的点最近的几个点来看

3. Which structure is more powerful in terms of expressiveness (i.e. it can represent a given Boolean function, accurately) — a single-layer perceptron or a 2-layer decision tree? (Hint: XOR)

a single-layer perceptron means logistic regression?

神经网络是可以拟合各种电路门的，具体可见coursera的machine learning课程或者udacity的deep learning课程

应该是a single-layer perceptron能力更强些

4. And, which one is more powerful — a 2 layer decision tree or a 2-layer neural network without any activation function? (Hint: non-linearity?)

2 layer decision tree能力更强，可以表征非线性函数，a 2-layer neural network without any activation function由于没有激活函数，只能表征线性函数

5. Can a neural network be used as a tool for dimensionality reduction? Explain how.

神经网络里有一种叫auto encoder的东西就是做这个的，也可以做数据压缩、图像压缩，详见udacity的deep learning课程，简单讲，原来的维度可以作为输入的维度，希望得到的较小的维度可以作为输出的维度，就这样实现了降维。

6. Everybody maligns and belittles the intercept term in a linear regression model. Tell me one of its utilities. (Hint: noise/garbage collector)

截距就是当前给出的变量和线性模型所不能解释的部分，也就是noise collector

7. LASSO regularization reduces coefficients to exact zero. Ridge regression reduces them to very small but non-zero value. Can you explain the difference intuitively from the plots of two simple function|x| and x²? (Hint: Those sharp corners in the |x| plot)

LASSO 是l1 regularization, Ridge是l2 regularization. x^2在0的地方是可导函数，而|x|在0的地方连续但不可导。这就造成了这种结果：LASSO regularization reduces coefficients to exact zero. Ridge regression reduces them to very small but non-zero value.

8. Let’s say that you don’t know anything about the distribution from which a data set (continuous valued numbers) came and you are forbidden to assume that it is Normal Gaussian. Show by simplest possible arguments that no matter what the true distribution is, you can guarantee that ~89% of the data will lie within +/- 3 standard deviations away from the mean (Hint: Markov’s Ph.D. adviser)

CHEBYSHEV’S THEOREM

As evidenced by Russian mathematician Pafnuty Chebyshev (1821-1894), irrespective of shape, the boundaries on the proportion of the data will lie a specified number of standard deviations from the mean.  A few
examples are as follows:

At least 75% of
 the data is within 2 standard deviations of the mean.
At least 89% of
 the data is within 3 standard deviations of the mean.
At least 95% of
 the data is within 4 1/2 standard deviations of the mean.
Chebyshev’s
 rule holds for both populations and samples and can be mathematically
 summarized as follows:

Percentage of Values Surrounding the Mean = 1 - (1/k^2)

For data that is
 bell-shaped and symmetrical (Normal):

Approximately
 68% of the data is within +/- 1 standard deviations of the mean.
Approximately
 95% of the data is within +/- 2 standard deviations of the mean.
More than 99% of
 the data is within +/- 3 standard deviations of the mean.
Studying the "Normal" or "Gaussian" probability distribution will help to demonstrate this concept in far greater detail and accuracy.

9. Majority of machine learning algorithms involve some kind of matrix manipulation like multiplication or inversion. Give a simple mathematical argument why a mini-batch version of such ML algorithm might be computationally more efficient than a training with full data set. (Hint: Time complexity of matrix multiplication…)

矩阵乘法的naive algorithm的时间复杂度是O(n^3), 假设是n*n与n*n矩阵的相乘.

for i=1 to n
   for j=1 to n    
     c[i][j]=0
     for k=1 to n
         c[i][j] = c[i][j]+a[i][k]*b[k][j]

There do exist algorithms that reduce this somewhat, but you're not likely to find an O(n^2) implementation. 

因为时间复杂度如此之大，n的大小就很关键，所以，mini-batch training is more efficient than full-batch training.

10. Don’t you think that a time series is a really simple linear regression problem with only one response variable and a single predictor — time? What’s the problem with a linear regression fit (not necessarily with a single linear term but even with polynomial degree terms) approach in case of a time series data? (Hint: Past is an indicator of future…)

对于时序数据，未来不仅取决于过去的一个时间点的数据，而是和一连串时间有关，并且是有先后顺序的一连串数据。如果未来只取决与一个时间点，一元线性回归可能是有效的，如果未来取决于多个时间点但不讲究先后顺序，每个时间点可以作为一个独立变量，多元线性回归可以尝试，但对于有先后顺序的时间序列，最好还是用cnn或者rnn，可以捕捉这种次序。

11. Show by simple mathematical argument that finding the optimal decision trees for a classification problem among all the possible tree structures, can be an exponentially hard problem.(Hint: How many trees are there in the jungle anyway?)

回归树和决策树不仅能是二叉树，也能是多叉树。对于用于分类的决策树，不妨认为就是二叉树（最简单的情况），如果深度为n的话，就有2^n种不同的树，所以，深度越大，计算越慢，exponentially hard。

12. Both decision trees and deep neural networks are non-linear classifier i.e. they separates the space by complicated decision boundary. Why, then, it is so much easier for us to intuitively follow a decision tree model vs. a deep neural network?

决策树使用的函数比较简单，是机制直观的分割函数，比较容易理解，而神经网络使用的是线性函数加激活函数，没有对应的明确的机制，所以很难理解。

13. Back-propagation is the workhorse of deep learning. Name a few possible alternative techniques to train a neural network without using back-propagation. (Hint: Random search…)

Back-propagation求的是loss function对各个系数的偏导，从对网络结构从后向前算出来的。然后由此用gradient descent来更新系数，也就是说，系数如何变是有方向、有规划的。当然，系数如何变也可以随机选择，只要得到的loss function比之前小就行，只是，这种方法的效率要比Back-propagation差。

14. Let’s say you have two problems — a linear regression and a logistic regression (classification). Which one of them is more likely to be benefited from a newly discovered super-fast large matrix multiplication algorithm? Why? (Hint: Which one is more likely to use a matrix manipulation?)

logistic regression，因为用的矩阵操作要更多，线性回归只涉及ax+b（多个样本的话就是矩阵操作了），而logistic regression除了ax+b，还涉及到1/(1+e^-z)。（矩阵中的每个元素都要做这种操作）

15. What is the impact of correlation among predictors on principal component analysis? How can you tackle it?

如果不同的predictors存在correlation，pca首先做的就是求协方差矩阵，有协方差就表明有相关，通过对这个矩阵求特征向量和特征值，就构建了线性无关的新的向量来作为predictor，把原来的向量x与特征向量相乘，就变换到了新的坐标系中。pca是用来降维的，所以也不是选全部的特征向量，而是特征值最大的k个特征向量，最终就能变换到一个n维空间中。

16. You are asked to build a classification model about meteorites impact with Earth (important project for human civilization). After preliminary analysis, you get 99% accuracy. Should you be happy? Why not? What can you do about it? (Hint: Rare event…)

就像影响股市价格的黑天鹅事件一样，小概率事件也是可能发生的，但是从模型上确实很难捕捉到，比如大的地震灾害

17. Is it possible capture the correlation between continuous and categorical variable? If yes, how?

If the response variable is continuous, categorical variable can be encoded as dummy variable, and we can use linear regression to capture the correlation.

Another way, we use the categorical variable as the response variable, then we can use logistic regresson or softmax regression to capture the correlation.

18. If you are working with gene expression data, there are often millions of predictor variables and only hundreds of sample. Give simple mathematical argument why ordinary-least-square is not a good choice for such situation if you to build a regression model. (Hint: Some matrix algebra…)

方程组中未知数的数目远大于方程的数目，会导致产生很多的解，约束条件不够，很难确定哪个解是正确的。

19. Explain why k-fold cross-validation does not work well with time-series model. What can you do about it? (Hint: Immediate past is a close indicator of future…)

在把训练集和测试集分开的时候，由于紧邻着的过去和未来相关性比较强，如果分别分在训练集和测试集，训练集和测试集就有关联，测试集就失去了测试的意义。解决的办法是，训练集取一段时间，测试集选离训练集时间较远的一段时间。

20. Simple random sampling of training data set into training and validation set works well for the regression problem. But what can go wrong with this approach for a classification problem? What can be done about it? (Hint: Are all classes prevalent to the same degree?)

不同的类别数目不同，抽样的时候，很难保证测试集和训练集中各个类的比例是一样的，或者说，训练集中训练出的模型不一定适用于整体以及测试集。解决的办法是，一个类一个类的来做train-validation split

21. Which is more important to you – model accuracy, or model performance?

都重要，目前来看，首先是model accuracy，model performance可以通过gpu以及cpu集群来想办法

22. If you could take advantage of multiple CPU cores, would you prefer a boosted-tree algorithm over a random forest? Why? (Hint: if you have 10 hands to do a task, you take advantage of it

boosted-tree和random forest都是基于decision tree的算法，decision tree是了解的，另外两种不太了解，如果有多核cpu的话，就看那种算法更适合并行计算。

23. Imagine your data set is known to be linearly separable and you have to guarantee the convergence and maximum number of iterations/steps of your algorithm (due to computational resource reason). Would you choose gradient descent in this case? What can you choose? (Hint: Which simple algorithm provides guarantee of finding solution?)

如果限定maximum number of iterations，计算资源有限的话，就不要用gradient descent了，对于线性问题，比如least square的loss function, 可以直接求导，然后求解方程组得到分类模型的参数值。

24. Let’s say you have a extremely small memory/storage. What kind of algorithm would you prefer — logistic regression or k-nearest neighbor? Why? (Hint: Space complexity)

看算法的空间复杂度，随着样本数的增长，数据之外的空间，logistic regression所需要的空间是不变的，只和模型的参数个数有关，而k-nearest neighbor所需的空间是随着样本数的增多而增多的，因为样本越多，需要算的距离就越多，接近线性增长。所以，内存小的话，更喜欢logistic regression。本质上讲，是因为logistic regression是parametric method，模型的参数是固定的，与数据无关的， k-nearest neighbor是nonparametric method，模型参数的数目适合数据的多少有关的。

25. To build a machine learning model initially you had 100 data points and 5 features. To reduce bias, you doubled the features to include 5 more variables and collected 100 more data points. Explain if this is a right approach? (Hint: There is a curse on machine learning. Have you heard about it?)

The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. 

当维度增加之后，数据处理起来会有各种各样的问题，最明显的一个，数据点要增加很多才能得到预期的效果，也就是说，高维模型对数据量的需求极大。


